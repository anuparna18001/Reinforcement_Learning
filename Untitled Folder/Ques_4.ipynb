{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX: BLACKJACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class game():\n",
    "    def __init__(self,HIT=0,STICK=1):\n",
    "        self.hit=HIT\n",
    "        self.stick=STICK\n",
    "        self.actions = [self.hit,self.stick]\n",
    "        self.policy_player = np.zeros(22, dtype=np.int)\n",
    "        for i in range(12, 20):\n",
    "            self.policy_player[i] = self.hit\n",
    "        self.policy_player[20] = self.stick\n",
    "        self.policy_player[21] = self.stick\n",
    "        self.policy_dealer = np.zeros(22)\n",
    "        for i in range(12, 17):\n",
    "            self.policy_dealer[i] = self.hit\n",
    "        for i in range(17, 22):\n",
    "            self.policy_dealer[i] = self.stick\n",
    "        \n",
    "    \n",
    "    #target policy\n",
    "    def target_policy(self, ace_player, player_sum, dealer_card):\n",
    "        return self.policy_player[player_sum]\n",
    "    \n",
    "    #behavior policy\n",
    "    def behavior_policy(self, ace_player, player_sum, dealer_card):\n",
    "        if np.random.binomial(1, 0.5) == 1:\n",
    "            return self.stick\n",
    "        return self.hit       \n",
    "            \n",
    "    # get a new card\n",
    "    def get_card(self):\n",
    "        card = np.random.randint(1, 14)\n",
    "        card = min(card, 10)\n",
    "        return card\n",
    "    \n",
    "    # get the value of a card (11 for ace).\n",
    "    def card_value(self,card_id):\n",
    "        if card_id == 1:\n",
    "            return 11\n",
    "        return card_id\n",
    "    \n",
    "    #initialize dealer\n",
    "    def initialize_dealer(self, dealer_card1, dealer_card2):\n",
    "        dealer_sum = self.card_value(dealer_card1) + self.card_value(dealer_card2)\n",
    "        ace_dealer = 1 in (dealer_card1, dealer_card2)\n",
    "        if dealer_sum > 21:\n",
    "            assert dealer_sum == 22\n",
    "            dealer_sum = dealer_sum - 10\n",
    "        assert dealer_sum <= 21\n",
    "        assert player_sum <= 21\n",
    "    \n",
    "    #start playing the game\n",
    "    def play(self, policy, initial_state=None, initial_action=None):\n",
    "        # player stats\n",
    "        player_sum = 0\n",
    "        player_moves = []\n",
    "        ace_player = False\n",
    "\n",
    "        # dealer stats\n",
    "        dealer_card1 = 0\n",
    "        dealer_card2 = 0\n",
    "        ace_dealer = False\n",
    "        \n",
    "        if initial_state is None:\n",
    "            # generate a random initial state for player\n",
    "            while player_sum < 12: \n",
    "                card = self.get_card()\n",
    "                player_sum = player_sum + self.card_value(card)\n",
    "                if player_sum > 21:\n",
    "                    assert player_sum == 22\n",
    "                    player_sum = player_sum - 10\n",
    "                else:\n",
    "                    ace_player |= (1 == card)\n",
    "            # initialize cards for dealer\n",
    "            dealer_card1 = self.get_card()\n",
    "            dealer_card2 = self.get_card()\n",
    "        else:\n",
    "            # use specified initial state for player and dealer\n",
    "            ace_player, player_sum, dealer_card1 = initial_state\n",
    "            dealer_card2 = self.get_card()\n",
    "        state = [ace_player, player_sum, dealer_card1]\n",
    "        dealer_sum = self.card_value(dealer_card1) + self.card_value(dealer_card2)\n",
    "        ace_dealer = 1 in (dealer_card1, dealer_card2)\n",
    "        if dealer_sum > 21:\n",
    "            assert dealer_sum == 22\n",
    "            dealer_sum = dealer_sum - 10\n",
    "        assert dealer_sum <= 21\n",
    "        assert player_sum <= 21\n",
    "        # player's turn\n",
    "        while True:\n",
    "            if initial_action is not None:\n",
    "                action = initial_action\n",
    "                initial_action = None\n",
    "            else:\n",
    "                action = self.policy(ace_player, player_sum, dealer_card1)\n",
    "            # track player's moves\n",
    "            player_moves.append([(ace_player, player_sum, dealer_card1), action])\n",
    "            if action == self.stick:\n",
    "                break\n",
    "            card = self.get_card()\n",
    "            ace_count = int(ace_player)\n",
    "            if card == 1:\n",
    "                ace_count = ace_count + 1\n",
    "            player_sum = player_sum + self.card_value(card)\n",
    "            # to avoid bursts\n",
    "            while player_sum > 21 and ace_count:\n",
    "                player_sum = player_sum - 10\n",
    "                ace_count = ace_count - 1\n",
    "            # player busts : looses\n",
    "            if player_sum > 21:\n",
    "                return state, -1, player_moves\n",
    "            assert player_sum <= 21\n",
    "            ace_player = (ace_count == 1)\n",
    "        # dealer's turn\n",
    "        while True:\n",
    "            action = self.policy_dealer[dealer_sum]\n",
    "            if action == self.stick:\n",
    "                break\n",
    "            new_card = self.get_card()\n",
    "            ace_count = int(ace_dealer)\n",
    "            if new_card == 1:\n",
    "                ace_count = ace_count + 1\n",
    "            dealer_sum = dealer_sum + self.card_value(new_card)\n",
    "            # avoid bursting\n",
    "            while dealer_sum > 21 and ace_count:\n",
    "                dealer_sum = dealer_sum - 10\n",
    "                ace_count = ace_count - 1\n",
    "            # dealer busts : looses\n",
    "            if dealer_sum > 21:\n",
    "                return state, 1, player_moves\n",
    "            ace_dealer = (ace_count == 1)\n",
    "            \n",
    "        # choses the winner\n",
    "        assert player_sum <= 21 and dealer_sum <= 21\n",
    "        if player_sum > dealer_sum:\n",
    "            return state, 1, player_moves\n",
    "        elif player_sum == dealer_sum:\n",
    "            return state, 0, player_moves\n",
    "        else:\n",
    "            return state, -1, player_moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policies():\n",
    "    def __init__(self):\n",
    "        self.g = game()\n",
    "    \n",
    "    #Greedy Policy\n",
    "    def behavior_policy(self, ace, player_sum, dealer_card):\n",
    "        ace = int(ace)\n",
    "        player_sum =player_sum - 12\n",
    "        dealer_card = dealer_card - 1\n",
    "        # get argmax of the average returns(s, a)\n",
    "        values_ = state_action_values[player_sum, dealer_card, ace, :] / \\\n",
    "                  state_action_pair_count[player_sum, dealer_card, ace, :]\n",
    "        return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "    \n",
    "    #Policy 1: on policy\n",
    "    def mc_on_policy(self,episodes):\n",
    "        states_ace = np.zeros((10, 10))\n",
    "        states_ace_count = np.ones((10, 10))\n",
    "        states_no_ace = np.zeros((10, 10))\n",
    "        states_no_ace_count = np.ones((10, 10))\n",
    "        for i in tqdm(range(0, episodes)):\n",
    "            _, reward, player_moves = self.g.play(self.g.target_policy)\n",
    "            for (ace, player_sum, dealer_card), _ in player_moves:\n",
    "                player_sum = player_sum - 12\n",
    "                dealer_card = dealer_card - 1\n",
    "                if ace:\n",
    "                    states_ace_count[player_sum, dealer_card] += 1\n",
    "                    states_ace[player_sum, dealer_card] += reward\n",
    "                else:\n",
    "                    states_no_ace_count[player_sum, dealer_card] += 1\n",
    "                    states_no_ace[player_sum, dealer_card] += reward\n",
    "        return states_ace / states_ace_count, states_no_ace / states_no_ace_count\n",
    "    \n",
    "    # Policy 2: Exploring Starts\n",
    "    def mc_es(self,episodes):\n",
    "        state_action_values = np.zeros((10, 10, 2, 2))\n",
    "        state_action_pair_count = np.ones((10, 10, 2, 2))\n",
    "        for episode in tqdm(range(episodes)):\n",
    "            initial_state = [bool(np.random.choice([0, 1])), np.random.choice(range(12, 22)), np.random.choice(range(1, 11))]\n",
    "            initial_action = np.random.choice(self.g.actions)\n",
    "            if episode:\n",
    "                current_policy = behavior_policy \n",
    "            else:\n",
    "                current_policy = target_policy\n",
    "            _, reward, moves = self.g.play(current_policy, initial_state, initial_action)\n",
    "            for (ace, player_sum, dealer_card), action in moves:\n",
    "                ace = int(ace)\n",
    "                player_sum = player_sum - 12\n",
    "                dealer_card = dealer_card - 1\n",
    "                # update values of state-action pairs\n",
    "                state_action_values[player_sum, dealer_card, ace, action] = state_action_values[player_sum, dealer_card, ace, action] + reward\n",
    "                state_action_pair_count[player_sum, dealer_card, ace, action] = state_action_pair_count[player_sum, dealer_card, ace, action] + 1\n",
    "        return state_action_values / state_action_pair_count\n",
    "    \n",
    "    # Policy 3: Off-Policy\n",
    "    def mc_off_policy(self,episodes):\n",
    "        initial_state = [True, 13, 2]\n",
    "        rhos = []     #importance sampling ratio\n",
    "        returns = []\n",
    "        for i in range(0, episodes):\n",
    "            _, reward, player_moves = self.g.play(behavior_policy_player, initial_state=initial_state)\n",
    "            # get the importance ratio\n",
    "            numerator = 1.0\n",
    "            denominator = 1.0\n",
    "            for (ace, player_sum, dealer_card), action in player_moves:\n",
    "                if action == target_policy_player(ace, player_sum, dealer_card):\n",
    "                    denominator = denominator * 0.5\n",
    "                else:\n",
    "                    numerator = 0.0\n",
    "                    break\n",
    "            rho = numerator / denominator\n",
    "            rhos.append(rho)\n",
    "            returns.append(reward)\n",
    "        rhos = np.asarray(rhos)\n",
    "        returns = np.asarray(returns)\n",
    "        weighted_returns = rhos * returns\n",
    "        weighted_returns = np.add.accumulate(weighted_returns)\n",
    "        rhos = np.add.accumulate(rhos)\n",
    "        ordinary_sampling = weighted_returns / np.arange(1, episodes + 1)\n",
    "        with np.errstate(divide='ignore',invalid='ignore'):\n",
    "            weighted_sampling = np.where(rhos != 0, weighted_returns / rhos, 0)\n",
    "        return ordinary_sampling, weighted_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class figure_plotting():\n",
    "    def __init__(self):\n",
    "        self.p = policies()\n",
    "        \n",
    "    def plot(self,state,title,axis,name):\n",
    "        for state, title, axis in zip(states, titles, axes):\n",
    "            fig = sns.heatmap(np.flipud(state), cmap=\"YlGnBu\", ax=axis, xticklabels=range(1, 11),yticklabels=list(reversed(range(12, 22))))\n",
    "            fig.set_ylabel('player sum', fontsize=30)\n",
    "            fig.set_xlabel('dealer showing', fontsize=30)\n",
    "            fig.set_title(title, fontsize=30)\n",
    "        plt.savefig(name)\n",
    "        plt.close()\n",
    "\n",
    "    def figure_5_1(self):\n",
    "        states_ace_1, states_no_ace_1 = self.p.mc_on_policy(10000)\n",
    "        states_ace_2, states_no_ace_2 = self.p.mc_on_policy(500000)\n",
    "        states = [states_ace_1, states_ace_2, states_no_ace_1, states_no_ace_2]\n",
    "        titles = ['Usable Ace, 10000 Episodes', 'Usable Ace, 500000 Episodes', 'No Usable Ace, 10000 Episodes', 'No Usable Ace, 500000 Episodes']\n",
    "        _, axes = plt.subplots(2, 2, figsize=(40, 30))\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "        axes = axes.flatten()\n",
    "        self.plot(state,title,axis,'figure_5_1.png')\n",
    "        \n",
    "    def figure_5_2(self):\n",
    "        state_action_values = self.p.mc_es(500000)\n",
    "        state_value_no_ace = np.max(state_action_values[:, :, 0, :], axis=-1)\n",
    "        state_value_ace = np.max(state_action_values[:, :, 1, :], axis=-1)\n",
    "        # get the optimal policy\n",
    "        action_no_ace = np.argmax(state_action_values[:, :, 0, :], axis=-1)\n",
    "        action_ace = np.argmax(state_action_values[:, :, 1, :], axis=-1)\n",
    "        images = [action_ace, state_value_ace, action_no_ace, state_value_no_ace]\n",
    "        titles = ['Optimal policy with usable Ace', 'Optimal value with usable Ace', 'Optimal policy without usable Ace', 'Optimal value without usable Ace']\n",
    "        _, axes = plt.subplots(2, 2, figsize=(40, 30))\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "        axes = axes.flatten()\n",
    "        self.plot(state,title,axis,'figure_5_2.png')\n",
    "\n",
    "    def figure_5_3(self):\n",
    "        true_value = -0.27726\n",
    "        episodes = 10000\n",
    "        runs = 100\n",
    "        error_ordinary = np.zeros(episodes)\n",
    "        error_weighted = np.zeros(episodes)\n",
    "        for i in tqdm(range(0, runs)):\n",
    "            ordinary_sampling_, weighted_sampling_ = self.p.mc_off_policy(episodes)\n",
    "            # get the squared error\n",
    "            error_ordinary += np.power(ordinary_sampling_ - true_value, 2)\n",
    "            error_weighted += np.power(weighted_sampling_ - true_value, 2)\n",
    "        error_ordinary /= runs\n",
    "        error_weighted /= runs\n",
    "        plt.plot(error_ordinary, label='Ordinary Importance Sampling')\n",
    "        plt.plot(error_weighted, label='Weighted Importance Sampling')\n",
    "        plt.xlabel('Episodes (log scale)')\n",
    "        plt.ylabel('Mean square error')\n",
    "        plt.xscale('log')\n",
    "        plt.legend()\n",
    "        plt.savefig('../images/figure_5_3.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 73529.33it/s]\n",
      "100%|██████████| 500000/500000 [00:07<00:00, 65172.15it/s]\n",
      "100%|██████████| 500000/500000 [00:29<00:00, 16857.75it/s]\n",
      "100%|██████████| 100/100 [00:19<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    f = figures()\n",
    "    f.figure_5_1()\n",
    "    f.figure_5_2()\n",
    "    f.figure_5_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
