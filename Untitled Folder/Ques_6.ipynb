{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 257.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 242.19it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 277.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 331.97it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 323.45it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 337.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 320.30it/s]\n",
      "100%|██████████| 100/100 [00:45<00:00,  2.01it/s]\n",
      "100%|██████████| 100/100 [00:43<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "class example():\n",
    "    def __init__(self):\n",
    "        self.values = np.zeros(7)\n",
    "        self.values[1:6] = 0.5\n",
    "        self.values[6] = 1\n",
    "        self.true_value = np.zeros(7)\n",
    "        self.true_value[1:6] = np.arange(1, 6) / 6.0\n",
    "        self.true_value[6] = 1\n",
    "        self.action_left = 0\n",
    "        self.action_right = 1\n",
    "        \n",
    "    def temporal_difference(self, values, alpha=0.1, batch=False):\n",
    "        state = 3\n",
    "        moves = [state]\n",
    "        rewards = [0]\n",
    "        while True:\n",
    "            old_state = state\n",
    "            if np.random.binomial(1, 0.5) == self.action_left:\n",
    "                state = state - 1\n",
    "            else:\n",
    "                state = state + 1\n",
    "            reward = 0\n",
    "            moves.append(state)\n",
    "            # TD update\n",
    "            if not batch:\n",
    "                self.values[old_state] += alpha * (reward + self.values[state] - self.values[old_state])\n",
    "            if state == 6 or state == 0:\n",
    "                break\n",
    "            rewards.append(reward)\n",
    "        return moves, rewards\n",
    "\n",
    "    def monte_carlo(self, values, alpha=0.1, batch=False):\n",
    "        state = 3\n",
    "        moves = [3]\n",
    "        while True:\n",
    "            if np.random.binomial(1, 0.5) == self.action_left:\n",
    "                state = state - 1\n",
    "            else:\n",
    "                state = state + 1\n",
    "            moves.append(state)\n",
    "            if state == 6:\n",
    "                returns = 1.0\n",
    "                break\n",
    "            elif state == 0:\n",
    "                returns = 0.0\n",
    "                break\n",
    "        if not batch:\n",
    "            for state_ in moves[:-1]:\n",
    "                # MC update step\n",
    "                self.values[state_] =  self.values[state_]  + alpha * (returns - self.values[state_])\n",
    "        return moves, [returns] * (len(moves) - 1)\n",
    "\n",
    "    # Example 6.2 left\n",
    "    def compute_state_value():\n",
    "        episodes = [0, 1, 10, 100]\n",
    "        current_values = np.copy(self.values)\n",
    "        plt.figure(1)\n",
    "        for i in range(episodes[-1] + 1):\n",
    "            if i in episodes:\n",
    "                plt.plot(current_values, label=str(i) + ' episodes')\n",
    "            self.temporal_difference(current_values)\n",
    "        plt.plot(TRUE_VALUE, label='true values')\n",
    "        plt.xlabel('state')\n",
    "        plt.ylabel('estimated value')\n",
    "        plt.legend()\n",
    "        \n",
    "    def rms_error(self):\n",
    "        td_alphas = [0.15, 0.1, 0.05]\n",
    "        mc_alphas = [0.01, 0.02, 0.03, 0.04]\n",
    "        episodes = 100 + 1\n",
    "        runs = 100\n",
    "        for i, alpha in enumerate(td_alphas + mc_alphas):\n",
    "            total_errors = np.zeros(episodes)\n",
    "            if i < len(td_alphas):\n",
    "                method = 'TD'\n",
    "                linestyle = 'solid'\n",
    "            else:\n",
    "                method = 'MC'\n",
    "                linestyle = 'dashdot'\n",
    "            for r in tqdm(range(runs)):\n",
    "                errors = []\n",
    "                current_values = np.copy(self.values)\n",
    "                for i in range(0, episodes):\n",
    "                    errors.append(np.sqrt(np.sum(np.power(self.true_values - current_values, 2)) / 5.0))\n",
    "                    if method == 'TD':\n",
    "                        self.temporal_difference(current_values, alpha=alpha)\n",
    "                    else:\n",
    "                        self.monte_carlo(current_values, alpha=alpha)\n",
    "                total_errors += np.asarray(errors)\n",
    "            total_errors /= runs\n",
    "            plt.plot(total_errors, linestyle=linestyle, label=method + ', alpha = %.02f' % (alpha))\n",
    "        plt.xlabel('episodes')\n",
    "        plt.ylabel('RMS')\n",
    "        plt.legend()\n",
    "\n",
    "    def batch_updating(self, method, episodes, alpha=0.001):\n",
    "        runs = 100\n",
    "        total_errors = np.zeros(episodes)\n",
    "        for r in tqdm(range(0, runs)):\n",
    "            current_values = np.copy(self.values)\n",
    "            errors = []\n",
    "            moves = []\n",
    "            rewards = []\n",
    "            for ep in range(episodes):\n",
    "                if method == 'TD':\n",
    "                    trajectory_, rewards_ = self.temporal_difference(current_values, batch=True)\n",
    "                else:\n",
    "                    trajectory_, rewards_ = self.monte_carlo(current_values, batch=True)\n",
    "                moves.append(trajectory_)\n",
    "                rewards.append(rewards_)\n",
    "                while True:\n",
    "                    updates = np.zeros(7)\n",
    "                    for trajectory_, rewards_ in zip(trajectories, rewards):\n",
    "                        for i in range(0, len(trajectory_) - 1):\n",
    "                            if method == 'TD':\n",
    "                                updates[trajectory_[i]] += rewards_[i] + current_values[trajectory_[i + 1]] - current_values[trajectory_[i]]\n",
    "                            else:\n",
    "                                updates[trajectory_[i]] += rewards_[i] - current_values[trajectory_[i]]\n",
    "                    updates *= alpha\n",
    "                    if np.sum(np.abs(updates)) < 1e-3:\n",
    "                        break\n",
    "                    current_values += updates\n",
    "                # calculate rms error\n",
    "                errors.append(np.sqrt(np.sum(np.power(current_values - self.true_values, 2)) / 5.0))\n",
    "            total_errors += np.asarray(errors)\n",
    "        total_errors /= runs\n",
    "        return total_errors\n",
    "\n",
    "    def example_6_2(self):\n",
    "        plt.figure(figsize=(10, 20))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        compute_state_value()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        rms_error()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig('example_6_2.png')\n",
    "        plt.close()\n",
    "\n",
    "    def figure_6_2(self):\n",
    "        episodes = 100 + 1\n",
    "        td_erros = batch_updating('TD', episodes)\n",
    "        mc_erros = batch_updating('MC', episodes)\n",
    "\n",
    "        plt.plot(td_erros, label='TD')\n",
    "        plt.plot(mc_erros, label='MC')\n",
    "        plt.xlabel('episodes')\n",
    "        plt.ylabel('RMS error')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig('figure_6_2.png')\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    e = example()\n",
    "    e.example_6_2()\n",
    "    e.figure_6_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
